{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name __version__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5d4559674f32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcaffe\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcaffe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcaffe_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcaffe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcaffe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/vashishtm/caffe/python/caffe/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpycaffe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSGDSolver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNesterovSolver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAdaGradSolver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRMSPropSolver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAdaDeltaSolver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAdamSolver\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_caffe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_mode_cpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset_mode_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_solver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_type_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_caffe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaffe_pb2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTEST\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name __version__"
     ]
    }
   ],
   "source": [
    "import caffe\n",
    "from caffe.proto import caffe_pb2\n",
    "from caffe import layers as L\n",
    "from caffe import params as P\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GPUId = 1 # can change this to 0,1,2\n",
    "caffe.set_mode_gpu()\n",
    "caffe.set_device(GPUId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training File Contents...\n",
      "train/a/abbey/00000966.jpg 0\n",
      "train/m/market/outdoor/00000553.jpg 65\n",
      "train/c/clothing_store/00000086.jpg 34\n",
      "train/c/candy_store/00000950.jpg 28\n",
      "train/s/ski_slope/00000361.jpg 84\n",
      "train/c/courtyard/00000730.jpg 42\n",
      "train/b/botanical_garden/00000121.jpg 21\n",
      "train/t/track/outdoor/00000331.jpg 95\n",
      "train/h/harbor/00000738.jpg 52\n",
      "train/b/bedroom/00000332.jpg 18\n"
     ]
    }
   ],
   "source": [
    "train_file_path = \"data/development_kit/data/train.txt\"\n",
    "val_file_path = \"data/development_kit/data/val.txt\"\n",
    "test_file_path = \"data/development_kit/data/test.txt\"\n",
    "\n",
    "print \"Training File Contents...\"\n",
    "content = [line.rstrip() for line in open(train_file_path)]\n",
    "random.shuffle(content)\n",
    "for x in content[:10]:\n",
    "    print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Data Parameters....\n",
    "CROP_SIZE = 98\n",
    "BATCH_SIZE = 100\n",
    "IMAGE_ROOT = \"data/images/\" #dont change this\n",
    "BIAS_CONSTANT = 0\n",
    "NET_PATH = \"train-net.prototxt\"\n",
    "VAL_NET_PATH = \"test-net.prototxt\"\n",
    "SOLVER_PATH = \"solver.prototxt\"\n",
    "\n",
    "#Learning Parameters\n",
    "weight_param = dict(lr_mult=1, decay_mult=1)\n",
    "bias_param   = dict(lr_mult=2, decay_mult=0)\n",
    "learned_param = [weight_param, bias_param]\n",
    "conv_filler = dict(type='xavier')\n",
    "const_filler = dict(type='constant', value=BIAS_CONSTANT)\n",
    "fc_filler = dict(type='xavier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_file_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a486cca79ded>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mplaces_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mplaces_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_file_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mval_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_file_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_file_path' is not defined"
     ]
    }
   ],
   "source": [
    "def get_data_layer(source,train=True):\n",
    "    mean = [104, 117, 123]  # per-channel mean of the BGR image pixels\n",
    "    transform_param = dict(mirror=train, crop_size=CROP_SIZE, mean_value=mean)\n",
    "    places_data, places_labels = L.ImageData(transform_param=transform_param,\n",
    "        source=source, root_folder=IMAGE_ROOT, shuffle=train,\n",
    "        batch_size=BATCH_SIZE, ntop=2)\n",
    "    return places_data,places_labels\n",
    "\n",
    "train_data,train_labels = get_data_layer(train_file_path,True)\n",
    "val_data,val_labels = get_data_layer(val_file_path,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_relu(bottom, ks, nout, stride=1, pad=0, group=1,weight_filler=conv_filler, bias_filler=const_filler):\n",
    "    conv = L.Convolution(bottom, kernel_size=ks, stride=stride,\n",
    "                         num_output=nout, pad=pad, group=group, param=learned_param,\n",
    "                         weight_filler=weight_filler, bias_filler=bias_filler)\n",
    "    relu = L.ReLU(conv, in_place=True)\n",
    "    return conv, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_relu(bottom, nout,weight_filler=fc_filler, bias_filler=const_filler):\n",
    "    fc = L.InnerProduct(bottom, num_output=nout, param=learned_param,weight_filler=weight_filler, bias_filler=bias_filler)\n",
    "    relu = L.ReLU(fc, in_place=True)\n",
    "    return fc, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool(bottom, ks, stride=1):\n",
    "    return L.Pooling(bottom, pool=P.Pooling.MAX, kernel_size=ks, stride=stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def places_net(data, labels, train=False,num_classes=100):\n",
    "    n = caffe.NetSpec()\n",
    "    n.data = data\n",
    "    n.conv1, n.relu1 = conv_relu(n.data, 11, 96, stride=4)\n",
    "    n.pool1 = max_pool(n.relu1, 3, stride=2)\n",
    "    n.conv2, n.relu2 = conv_relu(n.pool1, 5, 256, pad=2, group=2)\n",
    "    n.pool2 = max_pool(n.relu2, 3, stride=2)\n",
    "    n.conv3, n.relu3 = conv_relu(n.pool2, 3, 384, pad=1)\n",
    "    n.conv4, n.relu4 = conv_relu(n.relu3, 3, 384, pad=1, group=2)\n",
    "    n.conv5, n.relu5 = conv_relu(n.relu4, 3, 256, pad=1, group=2)\n",
    "    n.pool5 = max_pool(n.relu5, 3, stride=2)\n",
    "    n.fc6, n.relu6 = fc_relu(n.pool5, 1024)\n",
    "    n.drop6 = L.Dropout(n.relu6, in_place=True)\n",
    "    n.fc7, n.relu7 = fc_relu(n.drop6, 1024)\n",
    "    n.drop7 = L.Dropout(n.relu7, in_place=True)\n",
    "    preds = n.fc8 = L.InnerProduct(n.drop7, num_output=num_classes, param=learned_param)\n",
    "    if not train:\n",
    "        # Compute the per-label probabilities at test/inference time.\n",
    "        preds = n.probs = L.Softmax(n.fc8)\n",
    "    n.label = labels\n",
    "    n.loss = L.SoftmaxWithLoss(n.fc8, n.label)\n",
    "    n.accuracy_at_1 = L.Accuracy(preds, n.label)\n",
    "    n.accuracy_at_5 = L.Accuracy(preds, n.label,accuracy_param=dict(top_k=5))\n",
    "    if train:\n",
    "        net_path = NET_PATH\n",
    "    else:\n",
    "        net_paht = \n",
    "    with open(net_path,'w') as f:\n",
    "        f.write(str(n.to_proto()))\n",
    "    return net_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_net_file = places_net(train_data,train_labels,train=True)\n",
    "val_net_file = places_net(val_data,val_labels,train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def places_solver():\n",
    "    s = caffe_pb2.SolverParameter()\n",
    "    s.train_net = train_net_file\n",
    "    s.test_net.append(val_net_file)\n",
    "    \n",
    "    s.test_interval = 1000\n",
    "    s.test_iter.append(100)\n",
    "    s.iter_size = 1\n",
    "\n",
    "    # Solve using the stochastic gradient descent (SGD) algorithm.\n",
    "    # Other choices include 'Adam' and 'RMSProp'.\n",
    "    #s.type = 'SGD'\n",
    "\n",
    "    # The following settings (base_lr, lr_policy, gamma, stepsize, and max_iter),\n",
    "    # define the following learning rate schedule:\n",
    "    #   Iterations [  0, 20K) -> learning rate 0.01   = base_lr\n",
    "    #   Iterations [20K, 40K) -> learning rate 0.001  = base_lr * gamma\n",
    "    #   Iterations [40K, 50K) -> learning rate 0.0001 = base_lr * gamma^2\n",
    "    s.base_lr = 0.01\n",
    "    s.lr_policy = 'step'\n",
    "    s.gamma =  0.1\n",
    "    s.stepsize = 100000\n",
    "    s.max_iter = 450000\n",
    "\n",
    "    # Set other SGD hyperparameters. \n",
    "    #`momentum` = takes a weighted average of the current gradient and previous gradients -> more stable learning\n",
    "    # weight decay = regularizes learning; prevents overfitting\n",
    "    s.momentum = 0.9\n",
    "    s.weight_decay = 0.0005\n",
    "\n",
    "    # Display the current training loss and accuracy every `display` iterations.\n",
    "    s.display = 100\n",
    "    s.average_loss = 10\n",
    "    # Seed the RNG for deterministic results.\n",
    "    s.random_seed = 1\n",
    "\n",
    "    # Snapshots are files used to store networks we've trained\n",
    "    s.snapshot = s.stepsize // 2\n",
    "    s.snapshot_prefix = \"snapshot/\"\n",
    "    if not os.path.exists(s.snapshot_prefix):\n",
    "        os.makedirs(s.snapshot_prefix)\n",
    "    with open(SOLVER_PATH,'w') as f:\n",
    "        f.write(str(s))\n",
    "    return SOLVER_PATH  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solver_file = places_solver()\n",
    "solver = caffe.SGDSolver(solver_file) #change the invocation method to change solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_net(iters,disp):\n",
    "    outputs = sorted(solver.net.outputs)\n",
    "    def str_output(output):\n",
    "        value = solver.net.blobs[output].data\n",
    "        if output.startswith('accuracy'):\n",
    "            valstr = '%5.2f%%' % (100 * value, )\n",
    "        else:\n",
    "            valstr = '%6f' % value\n",
    "        return '%s = %s' % (output, valstr)\n",
    "    def disp_outputs(iteration, iter_pad_len=len(str(iters))):\n",
    "        metrics = '; '.join(str_output(o) for o in outputs)\n",
    "        return 'Iteration %*d: %s' % (iter_pad_len, iteration, metrics)\n",
    "    # We could just call `solver.solve()` rather than `step()`ing in a loop.\n",
    "    # (If we hadn't set GLOG_minloglevel = 3 at the top of this file, Caffe\n",
    "    # would display loss/accuracy information during training.)\n",
    "    previous_time = None\n",
    "    for iteration in xrange(iters):\n",
    "        solver.step(1)\n",
    "        if (disp > 0) and (iteration % disp == 0):\n",
    "            current_time = time.clock()\n",
    "            if previous_time is None:\n",
    "                benchmark = ''\n",
    "            else:\n",
    "                time_per_iter = (current_time - previous_time) / disp\n",
    "                benchmark = ' (%5f s/it)' % time_per_iter\n",
    "            previous_time = current_time\n",
    "            print disp_outputs(iteration), benchmark\n",
    "    # Print accuracy for last iteration.\n",
    "    solver.net.forward()\n",
    "    disp_outputs(iters)\n",
    "    #solver.net.save(snapshot_at_iteration(args.iters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_net(500,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
